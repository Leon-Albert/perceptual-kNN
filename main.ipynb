{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a08083",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c2eeb-840a-4946-9432-26cc1ca6a76f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06127a8c-52fc-456b-9503-eca3270ade1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from src.forward import *\n",
    "from src.knn import *\n",
    "from src.ftm import constants as FTM_constants\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.progress import track,Progress,BarColumn, TextColumn, SpinnerColumn, MofNCompleteColumn, TimeElapsedColumn, TimeRemainingColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44998c3d-8efb-4f7d-aaff-f4004bcab699",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Phi definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd00b1d3-0b58-4480-ad83-2ad9da31f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIRFilter(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, filter_type=\"hp\", coef=0.85, fs=44100, ntaps=101, plot=False):\n",
    "\n",
    "        \"\"\"Initilize FIR pre-emphasis filtering module.\"\"\"\n",
    "        super(FIRFilter, self).__init__()\n",
    "        self.filter_type = filter_type\n",
    "        self.coef = coef\n",
    "        self.fs = fs\n",
    "        self.ntaps = ntaps\n",
    "        self.plot = plot\n",
    "\n",
    "        import scipy.signal\n",
    "\n",
    "        if ntaps % 2 == 0:\n",
    "            raise ValueError(f\"ntaps must be odd (ntaps={ntaps}).\")\n",
    "\n",
    "        if filter_type == \"hp\":\n",
    "            self.fir = torch.nn.Conv1d(1, 1, kernel_size=3, bias=False, padding=1)\n",
    "            self.fir.weight.requires_grad = False\n",
    "            self.fir.weight.data = torch.tensor([1, -coef, 0]).view(1, 1, -1)\n",
    "        elif filter_type == \"fd\":\n",
    "            self.fir = torch.nn.Conv1d(1, 1, kernel_size=3, bias=False, padding=1)\n",
    "            self.fir.weight.requires_grad = False\n",
    "            self.fir.weight.data = torch.tensor([1, 0, -coef]).view(1, 1, -1)\n",
    "        elif filter_type == \"aw\":\n",
    "            # Definition of analog A-weighting filter according to IEC/CD 1672.\n",
    "            f1 = 20.598997\n",
    "            f2 = 107.65265\n",
    "            f3 = 737.86223\n",
    "            f4 = 12194.217\n",
    "            A1000 = 1.9997\n",
    "\n",
    "            NUMs = [(2 * np.pi * f4) ** 2 * (10 ** (A1000 / 20)), 0, 0, 0, 0]\n",
    "            DENs = np.polymul(\n",
    "                [1, 4 * np.pi * f4, (2 * np.pi * f4) ** 2],\n",
    "                [1, 4 * np.pi * f1, (2 * np.pi * f1) ** 2],\n",
    "            )\n",
    "            DENs = np.polymul(\n",
    "                np.polymul(DENs, [1, 2 * np.pi * f3]), [1, 2 * np.pi * f2]\n",
    "            )\n",
    "\n",
    "            # convert analog filter to digital filter\n",
    "            b, a = scipy.signal.bilinear(NUMs, DENs, fs=fs)\n",
    "\n",
    "            # compute the digital filter frequency response\n",
    "            w_iir, h_iir = scipy.signal.freqz(b, a, worN=512, fs=fs)\n",
    "\n",
    "            # then we fit to 101 tap FIR filter with least squares\n",
    "            taps = scipy.signal.firls(ntaps, w_iir, abs(h_iir), fs=fs)\n",
    "\n",
    "            # now implement this digital FIR filter as a Conv1d layer\n",
    "            self.fir = torch.nn.Conv1d(\n",
    "                1, 1, kernel_size=ntaps, bias=False, padding=ntaps // 2\n",
    "            )\n",
    "            self.fir.weight.requires_grad = False\n",
    "            self.fir.weight.data = torch.tensor(taps.astype(\"float32\")).view(1, 1, -1)\n",
    "\n",
    "        self.fir.weight.data = self.fir.weight.data.to(device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Calculate forward propagation.\n",
    "        Args:\n",
    "            input (Tensor): Predicted signal (B, #channels, #samples).\n",
    "            target (Tensor): Groundtruth signal (B, #channels, #samples).\n",
    "        Returns:\n",
    "            Tensor: Filtered signal.\n",
    "        \"\"\"\n",
    "        input = torch.nn.functional.conv1d(\n",
    "            input.unsqueeze(0).to(torch.float), self.fir.weight.data, padding=self.ntaps // 2\n",
    "        )\n",
    "        return input.squeeze(0).to(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251ae66-407e-4d72-b5d3-ba10952adeef",
   "metadata": {},
   "source": [
    "### Naive k neighbours search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e10daf5a-e2ef-423f-8bdc-2ab55e51920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_naive_tensor_wise_factory(Phi,logscale):\n",
    "    return functools.partial(dist_naive_tensor_wise,Phi=Phi,logscale=logscale)\n",
    "\n",
    "def dist_naive_tensor_wise(tensor_candidates,t_ref,Phi,logscale,show_progress=False):\n",
    "    dist_tensor = torch.zeros(tensor_candidates.size(dim=0)).to(device)\n",
    "    \n",
    "    #calculation of the audio for the reference node\n",
    "    phi_ref = Phi(rectangular_drum(t_ref, logscale=logscale,**FTM_constants))\n",
    "\n",
    "    progress = Progress(\n",
    "            TextColumn(\"[DISTANCES] Naive Method   \"),\n",
    "            SpinnerColumn(),\n",
    "            BarColumn(),\n",
    "            MofNCompleteColumn(),\n",
    "            TimeElapsedColumn(),\n",
    "            TimeRemainingColumn(),\n",
    "            transient=True\n",
    "    )\n",
    "    if show_progress:\n",
    "        iterator = progress.track(range(tensor_candidates.size(dim=0)))\n",
    "    else:\n",
    "        iterator = range(tensor_candidates.size(dim=0))\n",
    "        \n",
    "    with progress:\n",
    "        for j in iterator:\n",
    "            phi_node = Phi(rectangular_drum(tensor_candidates[j,:], logscale=logscale,**FTM_constants))\n",
    "            dist_tensor[j] = torch.sum(torch.pow(torch.subtract(phi_ref, phi_node), 2), dim=0)\n",
    "   \n",
    "    return dist_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3515fc4-0364-40c1-93e6-e420c076dd9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Approximated k neighbours search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33743ea-c02a-4190-b362-9029c32f327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create the M matrix\n",
    "\n",
    "# M(theta0) = grad(Phi o g)(theta0).T * grad(Phi o g)(theta0)\n",
    "# This return M = f(theta0)\n",
    "\n",
    "def M_from_G(G):\n",
    "    return torch.matmul(torch.transpose(G,0,1),G)\n",
    "\n",
    "def M_from_theta(theta, G):\n",
    "    return M_from_G(G(inputs=theta))\n",
    "\n",
    "def M_factory(logscale,Phi):\n",
    "    S_from_theta = pknn_forward_factory(logscale,Phi)\n",
    "    #This one is the only autograd fct we can run on our computers\n",
    "    G = functools.partial(torch.autograd.functional.jacobian, func=S_from_theta, create_graph=False,strategy=\"forward-mode\",vectorize=True) \n",
    "    M = functools.partial(M_from_theta,G=G)\n",
    "    return M\n",
    "\n",
    "# Then we define the distance function\n",
    "\n",
    "def dist_approximated(t_candidate, t_ref, M_t_ref):\n",
    "    return torch.matmul(torch.matmul(torch.transpose(M_t_ref,0,1),torch.sub(t_ref,t_candidate)),torch.sub(t_ref,t_candidate))\n",
    "\n",
    "def dist_approximated_tensor_wise(tensor_candidates,t_ref,M,show_progress=False): \n",
    "    dist_tensor = torch.zeros(tensor_candidates.size(dim=0)).to(device)\n",
    "    M_ref = M(t_ref).to(float)\n",
    "\n",
    "    progress = Progress(\n",
    "        TextColumn(\"[DISTANCES] Approx Method  \"),\n",
    "        SpinnerColumn(),\n",
    "        BarColumn(),\n",
    "        MofNCompleteColumn(),\n",
    "        TimeElapsedColumn(),\n",
    "        TimeRemainingColumn(),\n",
    "        transient=True\n",
    "    )\n",
    "    if show_progress:\n",
    "        iterator = progress.track(range(tensor_candidates.size(dim=0)))\n",
    "    else:\n",
    "        iterator = range(tensor_candidates.size(dim=0))\n",
    "    \n",
    "    with progress:\n",
    "        for j in iterator:\n",
    "            dist_tensor[j] = dist_approximated(tensor_candidates[j,:],t_ref,M_ref)\n",
    "\n",
    "    return dist_tensor\n",
    "\n",
    "def dist_approximated_tensor_wise_factory(Phi,logscale):\n",
    "    M = M_factory(logscale,Phi)\n",
    "    return functools.partial(dist_approximated_tensor_wise,M=M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1449854-d9e1-4f86-ba40-c69676fe25e5",
   "metadata": {},
   "source": [
    "### Create Parameters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67c2f51-28fe-4fad-a024-295d6041a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame and write it to a CSV file for later use\n",
    "\n",
    "def create_DF(bounds, subdiv, path):\n",
    "    \n",
    "    #Linspace of every parameters of size k\n",
    "    Dbase = np.zeros((subdiv,5))\n",
    "    for i in range(5):\n",
    "        Dbase[:,i] = np.linspace(bounds[1][i][0],bounds[1][i][1],subdiv)\n",
    "    baseDF = pd.DataFrame(data=Dbase,columns=bounds[0])\n",
    "\n",
    "    #Product of the linspaces to get all the possible combinations (size subdiv**5, will take time)\n",
    "    D = list(product(baseDF['omega'],baseDF['tau'],baseDF['p'],baseDF['d'],baseDF['alpha']))\n",
    "    DF = pd.DataFrame(data=D,columns=bounds[0])\n",
    "\n",
    "    DF.to_csv(path)\n",
    "    \n",
    "    return DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4499768",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288970fa-6ec7-43d5-9ca5-de42be0ebd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundaries\n",
    "\n",
    "bounds = [['omega', 'tau', 'p', 'd', 'alpha'],\n",
    " [(2.400247964468862, 3.798136579655672),\n",
    "  (0.0700188044714488, 0.7999966616122908),\n",
    "  (-4.999978530884291, -0.6989804486272966),\n",
    "  (-4.99983759075039, -0.5229983775344527),\n",
    "  (1.2362882382361523e-05, 0.9999649724709304)]]\n",
    "\n",
    "# Only run this to recreate the parameters CSV, this can take a long time to finish depending on the subdivision\n",
    "\n",
    "create_DF(bounds=bounds, subdiv=5, path='data/default_parameters.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eaf4aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4eddfdd6704db0a5f8bbf32a1877f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3125/3125</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3125/3125\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3124/3124</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3124/3124\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3123/3123</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3123/3123\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3122/3122</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3122/3122\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3121/3121</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3121/3121\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3120/3120</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3120/3120\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3119/3119</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3119/3119\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method     <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">3118/3118</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3118/3118\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[DISTANCES] Approx Method   <span style=\"color: #008000; text-decoration-color: #008000\">⠋</span> <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">   0/3117</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span> <span style=\"color: #008080; text-decoration-color: #008080\">-:--:--</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "[DISTANCES] Approx Method   \u001b[32m⠋\u001b[0m \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m   0/3117\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[36m-:--:--\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#Find the knn (Approx)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#dist = dist_naive_tensor_wise_factory(Phi, logscale)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#naive_id,naive_dist = find_neighbour(DatasetPath,id_refs,k,dist,show_progress=show_progress)\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#Find the knn (Approx)\u001b[39;00m\n\u001b[32m     17\u001b[39m dist = dist_approximated_tensor_wise_factory(Phi, logscale)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m approx_id,approx_dist = \u001b[43mfind_neighbour\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDatasetPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mid_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\smale\\Perso\\dev\\perceptual-kNN\\src\\knn.py:37\u001b[39m, in \u001b[36mfind_neighbour\u001b[39m\u001b[34m(DataFrame_path, ref_index_list, k, dist, show_progress)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i_ref \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         dist_tensor = \u001b[43mdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtensor_refs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# Add the k shortest distances to the matrix\u001b[39;00m\n\u001b[32m     39\u001b[39m         dist_tensor,sort_indices = torch.sort(dist_tensor)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mdist_approximated_tensor_wise\u001b[39m\u001b[34m(tensor_candidates, t_ref, M, show_progress)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         dist_tensor[j] = \u001b[43mdist_approximated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_candidates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dist_tensor\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mdist_approximated\u001b[39m\u001b[34m(t_candidate, t_ref, M_t_ref)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdist_approximated\u001b[39m(t_candidate, t_ref, M_t_ref):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.matmul(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM_t_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_candidate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,torch.sub(t_ref,t_candidate))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "DatasetPath = \"data/default_parameters.csv\"\n",
    "parameters_name = [\"omega\",\"tau\",\"p\",\"d\",\"alpha\"]\n",
    "logscale = True\n",
    "show_progress = True\n",
    "\n",
    "Phi = FIRFilter()\n",
    "k = 100\n",
    "nbr_ref = 3125\n",
    "\n",
    "id_refs = np.linspace(0,nbr_ref-1,nbr_ref).astype(int)\n",
    "\n",
    "#Find the knn (Approx)\n",
    "#dist = dist_naive_tensor_wise_factory(Phi, logscale)\n",
    "#naive_id,naive_dist = find_neighbour(DatasetPath,id_refs,k,dist,show_progress=show_progress)\n",
    "\n",
    "#Find the knn (Approx)\n",
    "dist = dist_approximated_tensor_wise_factory(Phi, logscale)\n",
    "approx_id,approx_dist = find_neighbour(DatasetPath,id_refs,k,dist,show_progress=show_progress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a792fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(approx_id)\n",
    "print(approx_dist)\n",
    "\n",
    "print(naive_id)\n",
    "print(naive_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ebcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the neighbours of the first ref point only to a CSV file \n",
    "\n",
    "naive_neighbours_id = naive_id[1,0:k]\n",
    "approx_neighbours_id = approx_id[1,0:k]\n",
    "\n",
    "data = torch.from_numpy(pd.read_csv(DatasetPath, index_col=0).to_numpy()).to(device).to(float)\n",
    "\n",
    "naive_neighbours = data[naive_neighbours_id,:].cpu()\n",
    "approx_neighbours = data[approx_neighbours_id,:].cpu()\n",
    "\n",
    "print('naive:')\n",
    "naive_DF = pd.DataFrame(naive_neighbours, columns=(parameters_name))\n",
    "print(naive_DF)\n",
    "naive_DF.to_csv(\"data/naive_knn.csv\")\n",
    "\n",
    "print('\\napprox:')\n",
    "approx_DF = pd.DataFrame(approx_neighbours, columns=(parameters_name))\n",
    "print(approx_DF)\n",
    "approx_DF.to_csv(\"data/approx_knn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b3207-e43d-4d6c-a821-8a632052eaa1",
   "metadata": {},
   "source": [
    "# Method characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc00b36-707b-4264-8a79-e88f2b03b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined the list of parameters\n",
    "approx_neighbours = []\n",
    "naive_neighbours = []\n",
    "\n",
    "parameters_name = ['omega', 'tau', 'p', 'd', 'alpha']\n",
    "\n",
    "#Recover all parameters\n",
    "data = pd.read_csv(\"data/naive_knn.csv\", index_col=0)\n",
    "data_size = data.size\n",
    "for i in range(int(data_size/5)):\n",
    "    #Get phi of the neighbour\n",
    "    parameterLine = data.iloc[[i]]\n",
    "    theta = np.array([ parameterLine[parameters_name[k]].iloc[0] for k in range(len(parameters_name)) ])\n",
    "    naive_neighbours.append(theta)\n",
    "\n",
    "#Recover all parameters\n",
    "data1 = pd.read_csv(\"data/approx_knn.csv\", index_col=0)\n",
    "data_size = data.size\n",
    "for i in range(int(data_size/5)):\n",
    "    #Get phi of the neighbour\n",
    "    parameterLine = data1.iloc[[i]]\n",
    "    theta = np.array([ parameterLine[parameters_name[k]].iloc[0] for k in range(len(parameters_name)) ])\n",
    "    approx_neighbours.append(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91658f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = dist_naive_tensor_wise_factory(Phi, logscale)\n",
    "\n",
    "print(dist(torch.tensor(naive_neighbours),naive_neighbours[0]))\n",
    "print(\"aaaa\")\n",
    "print(dist(torch.tensor(approx_neighbours),approx_neighbours[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa6d7f-bc85-4798-92c7-f093c33d1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTEN HERE the truth\n",
    "\n",
    "#Calulate audios from the parameters\n",
    "for i in range(len(naive_neighbours)):\n",
    "    print(\"Neighbours n°\",i+1)\n",
    "    audio = rectangular_drum(naive_neighbours[i],logscale=True,**FTM_constants)\n",
    "    ipd.display(ipd.Audio(data=audio.cpu().detach(), rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2098b0b-2e61-402f-af36-ae4920ad46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LISTEN HERE the approximation\n",
    "\n",
    "#Calulate audios from the parameters\n",
    "for i in range(len(approx_neighbours)):\n",
    "    print(\"Neighbours n°\",i+1)\n",
    "    audio = rectangular_drum(approx_neighbours[i], True,**FTM_constants)\n",
    "    ipd.display(ipd.Audio(data=audio.cpu().detach(), rate=44100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4135ebf-d568-4767-89ff-728eef1aa7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many neighbours to use \n",
    "\n",
    "precision = []\n",
    "\n",
    "for n in range(1,len(naive_neighbours)):\n",
    "\n",
    "    naive_neighbours_to_use = naive_neighbours[:n]\n",
    "    approx_neighbours_to_use = approx_neighbours[:n]\n",
    "\n",
    "    #Precision and recall\n",
    "\n",
    "    t_p = 0\n",
    "    f_p = 0\n",
    "\n",
    "    def inList(element,list_l):\n",
    "        for i in range(len(list_l)):\n",
    "            equal = True\n",
    "            for k in range(len(list_l[i])):\n",
    "                if(list_l[i][k]!= element[k]):\n",
    "                    equal = False\n",
    "            if (equal):\n",
    "                return True\n",
    "\n",
    "    for i in range(len(naive_neighbours_to_use)):\n",
    "        if (inList(approx_neighbours_to_use[i],naive_neighbours_to_use)):\n",
    "            t_p += 1\n",
    "        else:\n",
    "            f_p += 1\n",
    "\n",
    "    #print(\"Precision : \",t_p/len(naive_neighbours_to_use))\n",
    "    #print(\"Recall : \",f_p/len(naive_neighbours_to_use))\n",
    "\n",
    "    precision.append(t_p/len(naive_neighbours_to_use))\n",
    "\n",
    "x = np.linspace(1,len(naive_neighbours)-1,len(naive_neighbours)-1)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(x,precision)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
